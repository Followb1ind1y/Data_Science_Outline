# ðŸŽ¯ Data Science

All things data science: processes, best practices, setup guides, and more!

## 1. Tools for Data Science

- **Languages of Data Science:** Python, R Language, Sql
- **Data Science Tools:** [GitHub](https://github.com/Followb1ind1y/Data_Science), [Markdown & LaTex](https://towardsdatascience.com/write-markdown-latex-in-the-jupyter-notebook-10985edb91fd)
- Packages, APIs, Data Sets and Models

## 2. Data Analysis

- **Data Wrangling:** [Data preprocess](https://curiousily.com/posts/build-your-first-neural-network-with-pytorch/), Missing value, Data formatting, Data Normalization
- **Exploratory Data Analysis:** Descriptive statistic, GroupBy, Correlation, Chi-square
- **Model Evaluation and Refinement:** Overfitting, Underfitting, Model selection, Ridge Regression, Accuracy, Sensitivity, Specificity, Area under the ROC curve, F-measure

## 3. Data Visualization

- **Basic Visualization Tools:** [Visualization Tools(Matplotlib, Pandas, Seaborn)](https://github.com/Followb1ind1y/Data_Science_Outline/blob/main/Data_Visualization/01_Basic_Data_Visualization_Tools.ipynb): Area Plots, Histogram, Bar Charts, Pie Charts, Box Plots, Scatter Plots
- **Advanced Visualization Tools:** Waffle Charts, Word Clouds, Seaborn and Regression Plots, Maps and Visualizing Geospatial Data

## 4. Data Science Basics

- **Linear Algebra:** [Scalars](https://followb1ind1y.github.io/posts/applied_math_and_ml_basics/01_linear_algebra_for_ml/), Vectors, Matrices and Tensors, Multiplying Matrices and Vectors, Identity and Inverse Matrices, Linear Dependence and Span, Norms, Eigendecomposition, Singular Value Decomposition, The Trace Operator, The Determinant
- **Probability and Information Theory:** [Random Variables](https://followb1ind1y.github.io/posts/applied_math_and_ml_basics/02_probability_and_information_theory_for_ml/), Probability Distributions, Marginal Probability, Conditional Probability, Expectation, Variance and Covariance, Common Probability Distributions, Bayesâ€™ Rule
- **Numerical Computation:** [Overflow and Underflow](https://followb1ind1y.github.io/posts/applied_math_and_ml_basics/03_numerical_computation_for_ml/), Gradient-Based Optimization, Jacobian Hessian Matrices / Newtonâ€™s Methods, Constrained Optimization
- **Machine Learning Basics:** [Learning Algorithms](https://followb1ind1y.github.io/posts/applied_math_and_ml_basics/04_machine_learning_basics_for_ml/), Capacity, Overfitting and Underfitting, Cross-Validation, Estimators, Bias and Variance, Maximum Likelihood Estimation, Bayesian Statistics

## 5. Machine Learning

- **What is Machine Learning:** [Five Core Steps of Machine Learning](https://followb1ind1y.github.io/posts/machine_learning/01_what_is-_machine_learning_machine_learning/)
- **Linear Regression:** [Simple/Multiple Linear Regression](https://followb1ind1y.github.io/posts/machine_learning/02_linear_regression/), Non-Linear Regression
- **Logistic Regression:** [Logistic Regression](https://followb1ind1y.github.io/posts/machine_learning/03_logistic_regression/), Multinomial Logistic Regression, Ordinal Logistic Regression
- **Dimensionality Reduction Algorithms**: [LDA and QDA for Classification](https://followb1ind1y.github.io/posts/machine_learning/04_lda_and_qda_for_classification/) , [Principal Component Analysis (PCA)](https://followb1ind1y.github.io/posts/machine_learning/05_principal_component_analysis/), [Fisher's Linear Discriminant Analysis (FDA)](https://followb1ind1y.github.io/posts/machine_learning/06_fishers_linear_discriminant_analysis/)
- **K-Nearest Neighbors (KNN):** [K-Nearest Neighbors](https://followb1ind1y.github.io/posts/machine_learning/07_k_nearest_neighbors/)
- **Naive Bayes:** [Naive Bayes, Gaussian Naive Bayes](https://followb1ind1y.github.io/posts/machine_learning/08_naive_bayes/)
- **Support Vector Machine (SVM):** [Hard Margin SVM, Soft Margin SVM, Kernel Methods](https://followb1ind1y.github.io/posts/machine_learning/09_support_vector_machine/)
- **Decision Tree:** [Classification and Regression Trees(CART)](https://followb1ind1y.github.io/posts/machine_learning/10_decision_tree/)
- **Ensemble learning algorithms:** [Random Forest, Bagging](https://followb1ind1y.github.io/posts/machine_learning/11_bagging_and_random_forest/), [Boosting](https://followb1ind1y.github.io/posts/machine_learning/12_boosting/)
- **Clustering:** K-means, Fuzzy C-means, Hierarchical Clustering, DBSCAN

## 6. Deep Learning

- **Artificial Neural Network:** [Perceptron, Backpropagation](https://followb1ind1y.github.io/posts/deep_learning/01_perceptron_and_backpropagation/), [Radial Basis Function Neural Networks (RBN)](https://followb1ind1y.github.io/posts/deep_learning/02_radial_basis_function_neural_networks/)
- **Regularization for Deep Learning:** [Parameter Norm Penalties](https://followb1ind1y.github.io/posts/deep_learning/03_regularization_for_deep_learning/), Dataset Augmentation, Noise Injection, Early Stopping, Dropout
- **Optimization Methods for Deep Learning:** [Gradient Descent Optimization](https://followb1ind1y.github.io/posts/deep_learning/04_optimization_methods_for_deep_learning/), Gradient Descent Variants, Momentum, Nesterov Momentum, AdaGrad, RMSProp, Adam
- **Convolutional Neural Network:** [The Convolution Operation](https://followb1ind1y.github.io/posts/deep_learning/05_convolutional_neural_network/), Motivation, Pooling
- **Recurrent neural network**
- **Reinforcement Learning**
