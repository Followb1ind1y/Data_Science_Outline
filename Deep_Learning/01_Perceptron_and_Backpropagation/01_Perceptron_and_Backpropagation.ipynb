{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24fbcec8",
   "metadata": {},
   "source": [
    "# **01 - Neural Network: Perceptron and Backpropagation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea9929a",
   "metadata": {},
   "source": [
    "<img src=\"01_images/01_AI_ML_DL.PNG\" width=450px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e266b871",
   "metadata": {},
   "source": [
    "**Neural Networks** form the base of deep learning, which is a subfield of Machine Learning, where the structure of the human brain inspires the algorithms. Neural networks take input data, train themselves to recognize patterns found in the data, and then predict the output for a new set of similar data. Therefore, a neural network can be thought of as the functional unit of deep learning, which mimics the behavior of the human brain to solve complex data-driven problems.\n",
    "\n",
    "The first thing that comes to our mind when we think of \"neural networks\" is biology, and indeed, neural nets are inspired by our brains. In machine learning, the neurons’ **dendrites** refer to as input, and the nucleus process the data and forward the calculated output through the **axon**. In a biological neural network, the width (thickness) of dendrites defines the weight associated with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647e87a3",
   "metadata": {},
   "source": [
    "<img src=\"01_images/01_biological_neuron.jpeg\" width=700px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7c82cb",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "A **perceptron** is a neural network without any hidden layer. A perceptron only has an input layer and an output layer. Perceptrons can be represented graphically as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d31f284",
   "metadata": {},
   "source": [
    "<img src=\"01_images/01_Perceptron.PNG\" width=600px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02890c4",
   "metadata": {},
   "source": [
    "Where $x_{i}$ is the $i$-th feature of a sample and $\\beta_{i}$ is the $i$-th weight. $\\beta_{0}$ is defined as the bias. The bias alters the position of the decision boundary between the two classes. From a geometrical point of view, Perceptron assigns label \"1\" to elements on one side of $\\beta^{T}x+\\beta_{0}$ and label \"-1\" to elements on the other side. Define a cost function, $\\phi(\\beta,\\beta_{0})$, as a summation of the distance between all misclassified points and the hyperplane, or the decision boundary. To minimize the cost function, we need to estimate $\\beta$, $\\beta_{0}$. \n",
    "\n",
    "$$\n",
    "min_{\\beta,\\beta_{0}}\\phi(\\beta,\\beta_{0})={distance \\ of \\ all \\ misclassified \\ points}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a72909",
   "metadata": {},
   "source": [
    "<img src=\"01_images/01_decision_boundary.PNG\" width=250px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f07fd2",
   "metadata": {},
   "source": [
    "**1.** A hyperplane $L$ can be defined as:\n",
    "\n",
    "$$\n",
    "L=\\{x:f(x)= \\beta^{T}x+\\beta_{0}=0\\}\n",
    "$$\n",
    "\n",
    "* For any arbitrary points $x_{1}$ and $x_{2}$ on $L$, we have\n",
    "\n",
    "$$\n",
    "\\beta^{T}x_{1}+\\beta_{0}=0 \\\\\n",
    "\\beta^{T}x_{2}+\\beta_{0}=0 \\\\\n",
    "Such \\ that \\ \\beta^{T}(x_{1}-x_{2})=0 \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c7352e",
   "metadata": {},
   "source": [
    "**2.** For any $x_{0}$ on the hyperplane,\n",
    "\n",
    "$$\n",
    "\\beta^{T}x_{0}+\\beta_{0}=0 \\Rightarrow \\beta^{T}x_{0} = -\\beta_{0}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9650580b",
   "metadata": {},
   "source": [
    "**3.** We set $\\beta^{*}=\\frac{\\beta}{\\parallel\\beta\\parallel}$ as the unit normal vector of the hyperplane $L$. For simplicity we can call $\\beta^{*}$ norm vector. The distance of point $x$ to $L$ is given by\n",
    "\n",
    "$$\n",
    "\\beta^{*T}(x-x_{0}) = \\beta^{*T}x - \\beta^{*T}x_{0}= \\frac{\\beta^{T}x}{\\parallel\\beta\\parallel}+\\frac{\\beta_{0}}{\\parallel\\beta\\parallel} = \\frac{(\\beta^{T}x+\\beta_{0})}{\\parallel\\beta\\parallel}\n",
    "$$\n",
    "\n",
    "* Where $x_{0}$ is any point on $L$. Hence, $\\beta^{T}x+\\beta_{0}$ is proportional to the distance of the point $x$ to the hyperplane $L$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44fbee9",
   "metadata": {},
   "source": [
    "**4.** The distance from a misclassified data point $x_{i}$ to the hyperplane $L$ is\n",
    "\n",
    "$$\n",
    "d_{i}=-y_{i}(\\beta^{T}x_{i}+\\beta_{0})\n",
    "$$\n",
    "\n",
    "* Where $y_{i}$ is a target value, such that $y_{i}=1$ if $\\beta^{T}x_{i}+\\beta_{0}<0$, $y_{i}=-1$ if $\\beta^{T}x_{i}+\\beta_{0}>0$\n",
    "\n",
    "Since we need to find the distance from the hyperplane to the misclassified data points, we need to add a negative sign in front. When the data point is misclassified, $\\beta^{T}x_{i}+\\beta_{0}$ will produce an opposite sign of $y_{i}$. Since we need a positive sign for distance, we add a negative sign."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7349f203",
   "metadata": {},
   "source": [
    "### Perceptron Learning using Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d26e2a7",
   "metadata": {},
   "source": [
    "The gradient descent is an optimization method that finds the minimum of an objective function by incrementally updating its parameters in the negative direction of the derivative of this function. In our case, the objective function to be minimized is classification error and the parameters of this function are the weights associated with the inputs $\\beta$. The gradient descent algorithm updates the weights as follows:\n",
    "\n",
    "$$\n",
    "\\beta^{new} \\leftarrow \\beta^{old} - \\rho \\frac{\\partial Err}{\\partial \\beta}\n",
    "$$\n",
    "\n",
    "* Where $\\rho$ is called the learning rate.\n",
    "\n",
    "The classification error can be defined as the distance of misclassified observations to the decision boundary,\n",
    "\n",
    "$$\n",
    "D(\\beta) = -\\sum_{i\\in M}y_{i}\\beta^{T}x_{i}\n",
    "$$\n",
    "\n",
    "Where $M$ is the set of misclassified points. The quantity $y_{i}\\beta^{T}x_{i}$ will be negative if $x_{i}$\n",
    "is misclassified. By taking the derivative of $D(\\beta)$ with respect to $\\beta$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial D}{\\partial \\beta} &= - \\sum_{i\\in M}y_{i}x_{i} \\\\\n",
    "\\frac{\\partial D}{\\partial \\beta_{0}} &= - \\sum_{i\\in M}y_{i} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The update formula becomes\n",
    "\n",
    "$$\n",
    "\\beta^{new} \\leftarrow \\beta^{old} + \\rho \\sum_{i\\in M}y_{i}x_{i}\n",
    "$$\n",
    "\n",
    "Which is equivalent to incrementally updating $\\beta$ for each misclassified point $x_{i}$\n",
    "\n",
    "$$\n",
    "\\beta^{new} \\leftarrow \\beta^{old} + \\rho y_{i}x_{i}\n",
    "$$\n",
    "\n",
    "The intuition behind this update is that for misclassified point $x_{i}$, $\\beta$ should be changed in the direction that makes $x_{i}$ as close as possible to the right side. Figure 2 shows how $\\beta$ is updated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351363fa",
   "metadata": {},
   "source": [
    "<img src=\"01_images/01_GD.PNG\" width=350px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d8e719",
   "metadata": {},
   "source": [
    "#### Separability and Convergence\n",
    "\n",
    "The training set $D$ is said to be linearly separable if there exits a positive constant $\\gamma$ and a weight vector $\\beta$ such that $(\\beta^{T}x_{i}+\\beta_{0})y_{i}>\\gamma$ for all $1<i<n$. That is, if we say that $\\beta$ is the weight vector of Perceptron and $y_{i}$ is the true label of $x_{i}$, then the signd distance of the $x_{i}$ from $\\beta$ is greater than a positive constant $\\gamma$ for any $(x_{i},y_{i})\\in D$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e5f3bb",
   "metadata": {},
   "source": [
    "If data is linearly-separable, the solution is theoretically guranteed to converge to a separating hyperplane in a finite numver of iterations. In this situation the number of iterations depends on the learning rate and the margin. However, if the data is not linearly separable there is no guarantee that the algorithm converges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece9b8cb",
   "metadata": {},
   "source": [
    "#### Features\n",
    "\n",
    "* A Perceptron can only discriminate between two classes at a time.\n",
    "\n",
    "* When data is (linearly) separable, there are an infinite number of solutions depending on the starting point.\n",
    "\n",
    "* Even though convergence to a solution is guaranteed if the solution exists, the finite number of steps until convergence can be very large.\n",
    "\n",
    "* The smaller the gap between the two classes, the longer the time of convergence.\n",
    "\n",
    "* When the data is not separable, the algorithm will not converge (it should be stopped after N steps).\n",
    "\n",
    "* A learning rate that is too high will make the perceptron periodically oscillate around the solution unless additional steps are taken.\n",
    "\n",
    "* The L.S. compute a linear combination of feature of input and return the sign.\n",
    "\n",
    "* Learning rate affects the accuracy of the solution and the number of iterations directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574f19fa",
   "metadata": {},
   "source": [
    "## Feedforward Deep Networks\n",
    "\n",
    "Feedforward neural networks are artificial neural networks where the connections between units do not form a cycle. Feedforward neural networks were the first type of artificial neural network invented and are simpler than their counterpart, recurrent neural networks. They are called feedforward because information only travels forward in the network (no loops), first through the input nodes, then through the hidden nodes (if present), and finally through the output nodes.\n",
    "\n",
    "Feedforward neural network is a multistage regression or classification model typically represented by a graphical diagram. **Regression** usually produces **one** output unit $Y_{1}$ while for **$k$ - classification** there are **$k$** output units $Y_{1...k}$ with each $Y_{k}$ coded as 0 − 1 to represent the $k^{th}$ class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9192b3",
   "metadata": {},
   "source": [
    "<img src=\"01_images/01_FNN.PNG\" width=600px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d08c893",
   "metadata": {},
   "source": [
    "where $a_{i} = u \\cdot x$ and $z_{i} = \\phi(a_{i})$ which is a non-linear function with an example being $\\phi(a) = \\frac{1}{1+e^{−a}}$. The function $\\phi$ is called the activation function and is used in classification not regression.\n",
    "\n",
    "Feedforward deep networks, a.k.a. multilayer perceptrons (MLPs), are parametric function composed of several parametric function. Each layer of the network defines one of these sub-functions. Each layer (sub-function) has multiple inputs and multiple outputs. Each layer composed of many units (scalar output of the layer). We sometimes refer to each unit as a feature. Each unit is usually a simple transformation of its input. Also, the entire network can be very complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e516b41c",
   "metadata": {},
   "source": [
    "##  Backpropagation\n",
    "\n",
    "Back-propagation is the essence of neural net training. It is the method of fine-tuning the weights of a neural net based on the error rate obtained in the previous epoch (i.e., iteration). Proper tuning of the weights allows you to reduce error rates and to make the model reliable by increasing its generalization.\n",
    "\n",
    "Backpropagation is a short form for \"backward propagation of errors.\" It is a standard method of training artificial neural networks. This method helps to calculate the gradient of a loss function with respects to all the weights in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a844d8",
   "metadata": {},
   "source": [
    "<img src=\"01_images/01_backprob.PNG\" width=650px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac97504",
   "metadata": {},
   "source": [
    "$$\n",
    "Error = |\\hat{Y}-Y|^{2}, \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ a_{i} = \\sum_{l}z_{l}u_{il}, \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ z_{i} = \\sigma(a_{i}), \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\sigma(a) = \\frac{1}{1+e^{-a}}\n",
    "$$\n",
    "\n",
    "Take the derivative with respect to weight $u_{il}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Error}{\\partial u_{il}} = \\underbrace{\\frac{\\partial Error}{\\partial a_{i}}}_{\\delta_{i}\\ (Unknown)} \\cdot \\underbrace{\\frac{\\partial a_{i}}{\\partial u_{il}}}_{z_{l} \\ (Known)} \\\\ \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\delta_{i} = \\frac{\\partial Error}{\\partial a_{i}} &= \\sum_{j}\\underbrace{\\frac{\\partial Error}{\\partial a_{j}}}_{\\delta_{j}} \\cdot \\frac{\\partial a_{j}}{\\partial a_{i}} \\to (\\frac{\\partial a_{j}}{\\partial z_{i}}\\cdot \\frac{\\partial z_{i}}{\\partial a_{i}}) \\\\\n",
    "&= \\sum_{j}\\delta_{j} \\cdot u_{ji} \\cdot \\sigma'(a_{i}) = \\sigma'(a_{i})\\sum_{j}\\delta_{j} \\cdot u_{ji}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note that if $\\sigma(x)$ is the sigmoid function, then\n",
    "\n",
    "$$\n",
    "\\sigma'(x) = \\sigma(x)(1-\\sigma(x))\n",
    "$$\n",
    "\n",
    "Now considering $\\delta_{k}$ for the output layer:\n",
    "\n",
    "$$\n",
    "\\delta_{k} = \\frac{\\partial Error}{\\partial a_{k}} = \\frac{\\partial (y-\\hat{y})^{2}}{\\partial a_{k}} = -2(y-\\hat{y})  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ where \\ a_{k} = \\hat{y}\n",
    "$$\n",
    "\n",
    "The network weights are updated using the backpropagation algorithm when each training data point $x$ is fed into the feed forward neural network (FFNN).\n",
    "\n",
    "$$\n",
    "u_{il}^{new} \\leftarrow u_{il}^{old} - \\rho \\cdot \\frac{\\partial (y-\\hat{y})^{2}}{\\partial u_{il}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb3781c",
   "metadata": {},
   "source": [
    "### Backpropagation procedure\n",
    "\n",
    "\n",
    "1. First arbitrarily choose some random weights (preferably close to zero) for your network.\n",
    "\n",
    "\n",
    "2. Apply $x$ to the FFNN’s input layer, and calculate the outputs of all input neurons.\n",
    "\n",
    "\n",
    "3. Propagate the outputs of each hidden layer forward, one hidden layer at a time, and calculate the outputs of all hidden neurons.\n",
    "\n",
    "\n",
    "4. Once $x$ reaches the output layer, calculate the output(s) of all output neuron(s) given the outputs of the previous hidden layer.\n",
    "\n",
    "\n",
    "5. At the output layer, compute $\\delta_{k} = −2(y_{k} − \\hat{y}_{k} )$ for each output neuron(s).\n",
    "\n",
    "\n",
    "6. Compute each $\\delta_{i}$, starting from $i = k − 1$ all the way to the first hidden layer, where $\\delta_{i}=\\sigma'(a_{i})\\sum_{j}\\delta_{j} \\cdot u_{ji}$\n",
    "\n",
    "\n",
    "7. Compute $\\frac{\\partial (y-\\hat{y})^{2}}{\\partial u_{il}} = \\delta_{i}z_{l}$ for all weights $u_{il}$.\n",
    "\n",
    "\n",
    "8. Then update $u_{il}^{new} \\leftarrow u_{il}^{old} - \\rho\\cdot \\frac{\\partial (y-\\hat{y})^{2}}{\\partial u_{il}}$ for all weights $u_{il}$.\n",
    "\n",
    "\n",
    "9. Continue for next data points and iterate on the training set until weights converge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a44987",
   "metadata": {},
   "source": [
    "It is common to cycle through the all of the data points multiple times in order to reach convergence. An epoch represents one cycle in which you feed all of your datapoints through the neural network. It is good practice to randomized the order you feed the points to the neural network within each epoch; this can prevent your weights changing in cycles. The number of epochs required for convergence depends greatly on the learning rate & convergence requirements used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d8dd1e",
   "metadata": {},
   "source": [
    "## Implementing Propagation from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1626d219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d24ff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X --> input dataset of shape (input size, number of examples)\n",
    "# Y --> labels of shape (output size, number of examples)\n",
    "  \n",
    "W1 = np.random.randn(4, X.shape[0]) * 0.01\n",
    "b1 = np.zeros(shape =(4, 1))\n",
    "  \n",
    "W2 = np.random.randn(Y.shape[0], 4) * 0.01\n",
    "b2 = np.zeros(shape =(Y.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db165757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "591b78e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, W1, W2, b1, b2):\n",
    "  \n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "      \n",
    "    # here the cache is the data of previous iteration\n",
    "    # This will be used for backpropagation\n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "      \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd2934b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here Y is actual output\n",
    "def compute_cost(A2, Y):\n",
    "    m = Y.shape[1]\n",
    "    # implementing the above formula\n",
    "    cost_sum = np.multiply(np.log(A2), Y) + np.multiply((1 - Y), np.log(1 - A2))\n",
    "    cost = - np.sum(logprobs) / m\n",
    "      \n",
    "    # Squeezing to avoid unnecessary dimensions\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecbda43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(W1, b1, W2, b2, cache):\n",
    "   \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\"\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "  \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis = 1, keepdims = True)\n",
    "  \n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis = 1, keepdims = True)\n",
    "      \n",
    "    # Updating the parameters according to algorithm\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "  \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de3df62",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d218a2e3",
   "metadata": {},
   "source": [
    "[1] $\\ \\ \\ \\$ Odegua, R. (2021, April 8). Building a Neural Network From Scratch Using Python (Part 1). Medium. https://heartbeat.fritz.ai/building-a-neural-network-from-scratch-using-python-part-1-6d399df8d432. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568a0d5e",
   "metadata": {},
   "source": [
    "[2] $\\ \\ \\ \\$ Shukla, P., &amp; Iriondo, R. (2021, April 2). Neural Networks from Scratch with Python Code and Math in Detail- I. Medium. https://pub.towardsai.net/building-neural-networks-from-scratch-with-python-code-and-math-in-detail-i-536fae5d7bbf#3a44. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2 (default, Dec 21 2020, 15:06:04) \n[Clang 12.0.0 (clang-1200.0.32.29)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
