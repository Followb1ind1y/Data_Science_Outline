{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07f509e4",
   "metadata": {},
   "source": [
    "# **02 - Neural Network: Radial Basis Function Neural Networks (RBN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49e0191",
   "metadata": {},
   "source": [
    "In **Single Perceptron / Multi-layer Perceptron(MLP)**, we only have linear separability because they are composed of input and output layers(some hidden layers in MLP). We at least need one hidden layer to derive a non-linearity separation. Our **RBN** what it does is, it transforms the input signal into another form, which can be then feed into the network to get **linear separability**. RBN is structurally same as perceptron(MLP)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4202ebf",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"02_images/02_MLP_RBN.PNG\" width=400px/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8360bfa",
   "metadata": {},
   "source": [
    "RBNN is composed of **input**, **hidden**, and **output** layer. RBNN is **strictly limited** to have exactly **one** hidden layer. We call this hidden layer as **feature vector**. We apply **non-linear transfer function** to the feature vector before we go for classification problem. When we increase the dimension of the feature vector, the linear separability of feature vector increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c71dc66",
   "metadata": {},
   "source": [
    "## Network Structure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5b25cc0",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"02_images/02_RBN_Structure.PNG\" width=350px/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e717c04",
   "metadata": {},
   "source": [
    "### 1. Input :\n",
    "\n",
    "$$\n",
    "\\{(x_{i},y_{i})\\}_{i=1}^{n} \\ , \\ where \\ x_{i} \\subset \\mathbb{R}^{d} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c29ead",
   "metadata": {},
   "source": [
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "x_{11} \\ \\cdots \\ x_{1n} \\\\\n",
    "\\vdots \\ \\ddots \\ \\vdots \\\\\n",
    "x_{d1} \\ \\cdots \\ x_{dn} \\\\\n",
    "\\end{bmatrix}_{\\ d\\times n} = \n",
    "\\begin{bmatrix}\n",
    "\\vdots \\ \\ \\ \\vdots \\\\\n",
    "x_{1} \\ \\cdots \\  x_{n} \\\\\n",
    "\\vdots \\ \\ \\ \\vdots \\\\\n",
    "\\end{bmatrix}_{\\ d\\times n} \\ , \\\n",
    "Y =\n",
    "\\begin{bmatrix}\n",
    "y_{11} \\ \\cdots \\ y_{1n} \\\\\n",
    "\\vdots \\ \\ddots \\ \\vdots \\\\\n",
    "y_{k1} \\ \\cdots \\ y_{kn} \\\\\n",
    "\\end{bmatrix}_{\\ k\\times n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69624a2",
   "metadata": {},
   "source": [
    "### 2. Radial Basis Function\n",
    "\n",
    "* We define a $receptor = t$.\n",
    "* We draw confrontal maps around the $receptor$.\n",
    "* Gaussian Functions are generally used for Radian Basis Function(confrontal mapping). So we define the radial distance $r = \\parallel x- t \\parallel.$\n",
    "* There are many choices for the basis function. The commonly used is:\n",
    "\n",
    "$$\n",
    "\\phi_{j}(x_{i}) = e^{-|x_{i}\\ - \\ \\mu_{j}|^{2}}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64ebeff0",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"02_images/02_Radial_Function.jpeg\" width=350px/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d70eab",
   "metadata": {},
   "source": [
    "### 3. Output:\n",
    "\n",
    "$$\n",
    "y_{k}(x)=\\sum_{j=1}^{m}W_{jk}\\phi_{j}(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4063da",
   "metadata": {},
   "source": [
    "$$\n",
    "W =\n",
    "\\begin{bmatrix}\n",
    "w_{11} \\ \\cdots \\ w_{1k} \\\\\n",
    "\\vdots \\ \\ddots \\ \\vdots \\\\\n",
    "w_{m1} \\ \\cdots \\ w_{mk} \\\\\n",
    "\\end{bmatrix}_{\\ m\\times k} \\ , \\\n",
    "\\phi =\n",
    "\\begin{bmatrix}\n",
    "\\phi_{1}(x_{1}) \\ \\cdots \\ \\phi_{1}(x_{n}) \\\\\n",
    "\\vdots \\ \\ddots \\ \\vdots \\\\\n",
    "\\phi_{m}(x_{1}) \\ \\cdots \\ \\phi_{m}(x_{n}) \\\\\n",
    "\\end{bmatrix}_{\\ m\\times n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b60793",
   "metadata": {},
   "source": [
    "The output will be:\n",
    "\n",
    "$$\n",
    "Y = W^{T}\\phi\n",
    "$$\n",
    "\n",
    "* where $Y$ and $\\phi$ are known while $W$ is unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f314b06",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5580e783",
   "metadata": {},
   "source": [
    "$$\n",
    "\\psi = \\parallel Y - W^{T}\\phi \\ \\parallel^{2}\n",
    "$$\n",
    "\n",
    "$W$ can be computed by minimizing our objective function w.r.t $w$.\n",
    "\n",
    "$$\n",
    "min_{W}\\parallel Y - W^{T}\\phi \\ \\parallel^{2}\n",
    "$$\n",
    "\n",
    "This optimization problem can be solved in close form:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial W}\\parallel Y - W^{T}\\phi \\ \\parallel^{2} &= \\frac{\\partial}{\\partial W}Tr[(Y - W^{T}\\phi)^{T}(Y - W^{T}\\phi)] \\\\\n",
    "&= \\frac{\\partial}{\\partial W}Tr[Y^{T}Y+\\phi^{T}WW^{T}\\phi-Y^{T}W^{T}\\phi - \\phi^{T}WY] \\\\\n",
    "&= 0 + 2\\phi\\phi^{T}W - 2\\phi Y^{T} = 0 \\\\\n",
    "&\\Rightarrow \\phi\\phi^{T}W = \\phi Y^{T} \\\\\n",
    "&\\Rightarrow W = (\\phi\\phi^{T})^{-1}\\phi Y^{T}\n",
    "\\end{align*} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6848d5a",
   "metadata": {},
   "source": [
    "In RBF network the estimated function is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{Y} &= W^{T}\\phi \\\\\n",
    "&= ((\\phi\\phi^{T})^{-1}\\phi Y^{T})^{T}\\phi \\\\\n",
    "&= Y\\phi^{T}((\\phi\\phi^{T})^{-1})^{T}\\phi \\\\\n",
    "\\Rightarrow \\hat{Y}^{T} &= \\underbrace{\\phi^{T}(\\phi\\phi^{T})^{-1}\\phi}_{H} Y^{T} \\\\\n",
    "\\Rightarrow \\hat{Y}^{T} &= HY^{T}\n",
    "\\end{align*} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e9cd71",
   "metadata": {},
   "source": [
    "## Steinâ€™s unbiased risk estimator (SURE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6eb0fb",
   "metadata": {},
   "source": [
    "Assume $\n",
    "T = \\{(x_{i},y_{i})\\}_{i=1}^{n}\n",
    "$ be the training set.\n",
    "\n",
    "* **$f(\\cdot)$** $\\to$ True model\n",
    "* **$\\hat{f}(\\cdot)$** $\\to$ Estimated model\n",
    "* **err** $\\to$ Empirical error: $\\frac{1}{n}\\sum_{i=1}^{n}(\\hat{y}_{i}-y_{i})^{2}$\n",
    "* **Err** $\\to$ True error: $\\frac{1}{n}\\sum_{i=1}^{n}(\\hat{f}_{i}-f_{i})^{2}$\n",
    "* **$y$** $\\to$ Observations\n",
    "\n",
    "Also assume\n",
    "\n",
    "$$\n",
    "y_{i} = f(x_{i}) + \\varepsilon_{i} \\ , \\ \\ \\ \\ Where \\ \\varepsilon_{i} \\sim N(0,\\sigma^{2})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35b3034",
   "metadata": {},
   "source": [
    "For point $(x_{0},y_{0})$ we are interested in\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "E[(\\hat{y}_{0}-y_{0})^{2}] &= E[(\\hat{f}_{0}-f_{0}-\\varepsilon_{0})^{2}] \\\\\n",
    "&= E[((\\hat{f}_{0}-f_{0})-\\varepsilon_{0})^{2}] \\\\\n",
    "&= E[(\\hat{f}_{0}-f_{0})^{2}+\\varepsilon_{0}^{2}-2\\varepsilon_{0}(\\hat{f}_{0}-f_{0})] \\\\\n",
    "&= E[(\\hat{f}_{0}-f_{0})^{2}]+E[\\varepsilon_{0}^{2}]-2E[\\varepsilon_{0}(\\hat{f}_{0}-f_{0})] \\\\\n",
    "&= E[(\\hat{f}_{0}-f_{0})^{2}]+\\sigma^{2}-2E[\\varepsilon_{0}(\\hat{f}_{0}-f_{0})] \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a796431",
   "metadata": {},
   "source": [
    "### Case 1\n",
    "\n",
    "Assume $(x_{0},y_{0})\\notin T$. \n",
    "\n",
    "In this case, since $\\hat{f}$ is estimated only based on points in training set, therefore it is completely independent from $(x_{0},y_{0})$\n",
    "\n",
    "$$\n",
    "E[\\varepsilon_{0}(\\hat{f}_{0}-f_{0})] = 0 \\\\\n",
    "\\Rightarrow E[(\\hat{y}_{0}-y_{0})^{2}] = E[(\\hat{f}_{0}-f_{0})^{2}]+\\sigma^{2}\n",
    "$$\n",
    "\n",
    "If summing up all $m$ points that are not in $T$.\n",
    "\n",
    "$$\n",
    "\\underbrace{\\sum_{i=1}^{m}(\\hat{y}_{i}-y_{i})^{2}}_{err}= \\underbrace{\\sum_{i=1}^{m}(\\hat{f}_{i}-f_{i})^{2}}_{Err}+ m\\sigma^{2} \\\\\n",
    "err = Err + m\\sigma^{2} \\\\\n",
    "Err = err - m\\sigma^{2}\n",
    "$$\n",
    "\n",
    "Empirical error ($err$) is a good estimator of true error ($Err$) if the point $(x_{0},y_{0})$ is not in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84d4946",
   "metadata": {},
   "source": [
    "### Case 2\n",
    "\n",
    "Assume $(x_{0},y_{0})\\in T$. Then $E[\\varepsilon_{0}(\\hat{f}_{0}-f_{0})] \\neq 0$\n",
    "\n",
    "**Stein's Lemma**: If $x \\sim N(\\theta,\\sigma^{2})$ and g(x) differentiable. Then,\n",
    "\n",
    "$$\n",
    "    E[g(x)(x-\\theta)]=\\sigma^{2}E[\\frac{\\partial g(x)}{\\partial x}]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "E[\\underbrace{\\varepsilon_{0}}_{(x-\\theta)}\\underbrace{(\\hat{f}_{0}-f_{0})}_{g(x)}] &=\\sigma^{2}E[\\frac{\\partial (\\hat{f}_{0}-f_{0})}{\\partial \\varepsilon_{0}}] \\\\\n",
    "&= \\sigma^{2}E[\\frac{\\partial \\hat{f}_{0}}{\\partial \\varepsilon_{0}}-\\underbrace{\\frac{\\partial f_{0}}{\\partial \\varepsilon_{0}}}_{0}] \\\\\n",
    "&= \\sigma^{2}E[\\frac{\\partial \\hat{f}_{0}}{\\partial \\varepsilon_{0}}] \\\\\n",
    "&= \\sigma^{2}E[\\frac{\\partial \\hat{f}_{0}}{\\partial y_{0}}\\cdot\\underbrace{\\frac{\\partial y_{0}}{\\partial \\varepsilon_{0}}}_{1}] \\\\\n",
    "&= \\sigma^{2}E[\\frac{\\partial \\hat{f}_{0}}{\\partial y_{0}}] \\\\\n",
    "&= \\sigma^{2}E[D_{0}]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "E[(\\hat{y}_{0}-y_{0})^{2}] = E[(\\hat{f}_{0}-f_{0})^{2}]+\\sigma^{2} - 2\\sigma^{2}E[D_{0}]\n",
    "$$\n",
    "\n",
    "Sum over all $n$ data points:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\sum_{i=1}^{n}(\\hat{y}_{i}-y_{i})^{2}}_{err}= \\underbrace{\\sum_{i=1}^{n}(\\hat{f}_{i}-f_{i})^{2}}_{Err}+ n\\sigma^{2} -2\\sigma^{2}\\sum_{i=1}^{n}D_{i} \\\\\n",
    "Err = err - n\\sigma^{2}+\\underbrace{2\\sigma^{2}\\sum_{i=1}^{n}D_{i}}_{Complexity \\ of \\ model} \\Rightarrow Steinâ€™s \\ Unbiased \\ Risk \\ Estimator \\ (SURE)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f785c3",
   "metadata": {},
   "source": [
    "## Coplexity control for RBN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9d48480",
   "metadata": {},
   "source": [
    "Let's apply **SURE** to **RBF**. Then **SURE** will be:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Err &= err - n\\sigma^{2}+2\\sigma^{2}\\sum_{i=1}^{n}H_{ii} \\\\\n",
    "&= err - n\\sigma^{2}+2\\sigma^{2}Tr(H) \\\\\n",
    "&= err - n\\sigma^{2}+2\\sigma^{2}Tr[\\phi^{T}(\\phi\\phi^{T})^{-1}\\phi] \\\\\n",
    "&= err - n\\sigma^{2}+2\\sigma^{2}Tr[\\underbrace{\\phi\\phi^{T}(\\phi\\phi^{T})^{-1}}_{I\\to m\\times m}] \\\\\n",
    "&= err - n\\sigma^{2}+2\\sigma^{2}Tr[I_{m}] \\\\\n",
    "&= err - n\\sigma^{2}+2\\sigma^{2}m \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For computing SURE, we need to know the value of $\\sigma$. But we do not know it. Therefore we need to estimate it.\n",
    "\n",
    "$$\n",
    "\\sigma^{2} = \\frac{\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^{2}}{n-1}\n",
    "$$\n",
    "\n",
    "is the function of complexity (more complex, smaller $\\sigma^{2}$), in practice, we do not consider the $\\hat{y}$ to be the function of complexity and instead we consider it to be a low bias and high variance estimation (for example a line). With this assumption the $\\sigma$ will be considered to be constant and independent from the complexity of model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a7d656",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77ef2ea",
   "metadata": {},
   "source": [
    "[1]$\\ \\ \\ \\$ McCormick, C. (2013, August 15). Radial Basis Function Network (RBFN) Tutorial. Radial Basis Function Network (RBFN) Tutorial Â· Chris McCormick. https://mccormickml.com/2013/08/15/radial-basis-function-network-rbfn-tutorial/. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2 (default, Dec 21 2020, 15:06:04) \n[Clang 12.0.0 (clang-1200.0.32.29)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
