{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e33e698",
   "metadata": {},
   "source": [
    "# **03 - Convolutional Neural Network (CNN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e575398",
   "metadata": {},
   "source": [
    "A Convolutional Neural Network (ConvNet/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other. The pre-processing required in a ConvNet is much lower as compared to other classification algorithms. While in primitive methods filters are hand-engineered, with enough training, ConvNets have the ability to learn these filters/characteristics.\n",
    "\n",
    "The architecture of a ConvNet is analogous to that of the connectivity pattern of Neurons in the Human Brain and was inspired by the organization of the Visual Cortex. Individual neurons respond to stimuli only in a restricted region of the visual field known as the Receptive Field. A collection of such fields overlap to cover the entire visual area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ba61ea",
   "metadata": {},
   "source": [
    "<img src=\"03_images/03_CNN_Overview.jpeg\" width=700px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d64c510",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "\n",
    "Suppose we have two **signals** $x$ and $w$, which you can think of as arrays, with elements\n",
    "denoted as $x[t]$ and so on. As you can guess based on the letters, you can think of $x$ as an input signal (such as a waveform or an image) and $w$ as a set of weights, which we’ll refer to as a **filter** or **kernel**. Normally the\n",
    "signals we work with are finite in extent, but it is sometimes convenient to treat them as infinitely large by treating the values as zero everywhere else; this is known as **zero padding**.\n",
    "\n",
    "This operation is called convolution.\n",
    "\n",
    "$$\n",
    "s(t) = \\int x(a)w(t-a)da \\\\\n",
    "$$\n",
    "\n",
    "The convolution operation is typically denoted with an asterisk:\n",
    "\n",
    "$$\n",
    "s(t) = (x * w)(t) \\\\\n",
    "$$\n",
    "\n",
    "Let’s start with the one-dimensional case. The convolution of $x$ and $w$, denoted $x * w$, is a signal with entries given by\n",
    "\n",
    "$$\n",
    "s(t) = (x * w)[t] = \\sum_{a}x[a]w[t-a] \\\\\n",
    "$$\n",
    "\n",
    "There are two ways to think about this equation. The first is **translateand-scale**: the signal $x ∗ w$ is composed of multiple copies of $x$, translated and scaled by various amounts according to the entries of $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3537134",
   "metadata": {},
   "source": [
    "<img src=\"03_images/03_Figure_1.PNG\" width=500px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d2c4cf",
   "metadata": {},
   "source": [
    "A second way to think about it is **flip-and-filter**. Here we generate each of the entries of $x ∗ w$ by flipping $w$, shifting it, and taking the dot product with $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4fcebc",
   "metadata": {},
   "source": [
    "<img src=\"03_images/03_Figure_2.PNG\" width=500px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc2c795",
   "metadata": {},
   "source": [
    "In practice, we often use convolutions over more than one axis at a time.\n",
    "\n",
    "$$\n",
    "s[i,j] = (I * K)[i,j]=\\sum_{m}\\sum_{n}I[m,n]K[i-m,j-n] \\\\\n",
    "$$\n",
    "\n",
    "* The input is usually a multidimensional array of data.\n",
    "* The kernel is usually a multidimensional array of parameters that should be learned.\n",
    "* We assume that these functions are zero everywhere but the finite set of points for which we store the values.\n",
    "* We can implement the infinite summation as a summation over a finite number of array elements.\n",
    "\n",
    "Convolution is commutative\n",
    "\n",
    "$$\n",
    "s[i,j] = (I * K)[i,j]=\\sum_{m}\\sum_{n}I[i-m,j-n]K[m,n] \\\\\n",
    "$$\n",
    "\n",
    "Cross-correlation,\n",
    "\n",
    "$$\n",
    "s[i,j] = (I * K)[i,j]=\\sum_{m}\\sum_{n}I[i+m,j+n]K[m,n] \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f6927f",
   "metadata": {},
   "source": [
    "<img src=\"03_images/03_Convolved_Feature.gif\" width=400px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1854bff0",
   "metadata": {},
   "source": [
    "### Properties of convolution\n",
    "\n",
    "Now that we’ve seen some examples of convolution, let’s note some useful properties. First of all, it behaves like multiplication, in that it’s **commutative** and associative: \n",
    "\n",
    "$$\n",
    "u * v = v * u \\\\\n",
    "(u * v) * w = u * (v * w) \\\\\n",
    "$$\n",
    "\n",
    "Another useful property of convolution is that it is **linear**:\n",
    "\n",
    "$$\n",
    "(ax + bx') * w = ax * w + bx' * w \\\\\n",
    "x * (aw + bw') = ax * w + bx * w' \\\\\n",
    "$$\n",
    "\n",
    "This is convenient, because linear operations are often easier to deal with. But it also shows an inherent limit to convolution: if you have a neural net which computes lots of convolutions in sequence, it can still only compute linear functions. In order to compute more complex operations, we’ll need to apply some sort of nonlinear activation function in each layer.\n",
    "\n",
    "One last property of convolution is that it’s **equivariant** to translation. This means that if we shift, or translate, $x$ by some amount, then the output $x∗w$ is shifted by the same amount. This is a useful property in the context of neural nets, because it means the network’s computations behave in a well-defined way as we transform the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc257ea",
   "metadata": {},
   "source": [
    "### Convolutional feature detection\n",
    "\n",
    "As alluded to above, convolutions are even more powerful when they’re paired with nonlinearities. A sequence of convolutions can only compute a linear function, but a sequence of convolutions alternated with nonlinearities can do fancier things. E.g., consider the following sequence of operations:\n",
    "\n",
    "1. Convolve the image with a horizontal edge filter.\n",
    "2. Apply the linear rectification nonlinearity\n",
    "\n",
    "$$\n",
    "\\phi(z) = \\begin{cases}\n",
    "z \\ \\ if \\ z>0 \\\\\n",
    "0 \\ \\ if \\ z\\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "3. Blur the result\n",
    "\n",
    "This sequence of steps, gives a map of horizontalness in various parts of an image; the same can be done for verticalness. You can hopefully imagine this being a useful feature for further processing. Because the resulting output can be thought of as a map of the feature strength over parts of an image, we refer to it as a **feature map**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8240601",
   "metadata": {},
   "source": [
    "<img src=\"03_images/03_Feature_Map.PNG\" width=400px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ecae0d",
   "metadata": {},
   "source": [
    "## Convolution layers\n",
    "\n",
    "We just saw that a convolution, followed by a nonlinear activation function, followed by another convolution, could compute something interesting. This motivates the **convolution layer**, a neural net layer which computes convolutions followed by a nonlinear activation function. Since convolution layers can be thought of as doing feature detection, they’re sometimes referred to as **detection layers**. First, let’s see how we can think about convolution in terms of units and connections.\n",
    "\n",
    "Confusingly, the way they’re standardly defined, convolution layers don’t actually compute convolutions, but a closely related operation called **filtering**:\n",
    "\n",
    "$$\n",
    "(x \\star w)[t] = \\sum_{a}x[t+a]w[a] \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d1cbef",
   "metadata": {},
   "source": [
    "Like the name suggests, filtering is essentially like flip-and-filter, but without the flipping. (i.e., $x * w = x \\star flip(w)$.) The two operations are basically equivalent — the difference is just a matter of how the filter (or kernel) is represented.\n",
    "\n",
    "In the above example, we computed a single feature map, but just as we normally use more than one hidden unit in fully connected layers, convolution layers normally compute multiple feature maps $z_{1}, . . . , z_{M}$. The input layers also consist of multiple feature maps $x_{1}, . . . , x_{D}$; these could be different color channels of an RGB image, or feature maps computed by another convolution layer. There is a separate filter $w_{ij}$ associated with each pair of an input and output feature map. The activations are computed as follows:\n",
    "\n",
    "$$\n",
    "z_{i} = \\sum_{j}x_{j} \\star w_{ij} \\\\\n",
    "h_{i} = \\phi(z_{i}) \\\\\n",
    "$$\n",
    "\n",
    "The activation function $\\phi$ is applied elementwise.\n",
    "\n",
    "We can think about filtering as a layer of a neural network by thinking of the elements of $x$ and $x * w$ as units, and the elements of $w$ as connection weights. Such an interpretation is visualized in Figure 6 for a one-dimensional example. Each of the units in this network computes its activations in the standard way, i.e. by summing up each of the incoming units multiplied by their connection weights. This shows that a convolution layer is like a fully connected layer, except with two additional features:\n",
    "\n",
    "* **Sparse connectivity**: not every input unit is connected to every output unit.\n",
    "* **Weight sharing**: the network’s weights are each shared between multiple connections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da266067",
   "metadata": {},
   "source": [
    "<img src=\"03_images/03_Convolution_Layer.PNG\" width=600px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19032fe6",
   "metadata": {},
   "source": [
    "Missing connections can be thought of as connections with weight 0. This highlights an important fact: any function computed by a convolution layer can be computed by a fully connected layer. This means convolution layers don’t increase the representational capacity, relative to a fully connected layer with the same number of input and output units. But they can reduce the numbers of weights and connections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104905cc",
   "metadata": {},
   "source": [
    "## Pooling layers\n",
    "\n",
    "We observed that a neural network’s classifications ought to be invariant to small transformations of an image, such as shifting it by a few pixels. In order to achieve invariance, we introduce another kind of layer: the pooling layer. Pooling layers summarize (or compress) the feature maps of the previous layer by computing a simple function over small regions of the image. Most commonly, this function is taken to be the maximum, so the operation is known as max-pooling.\n",
    "\n",
    "Suppose we have input feature maps $x_{1}, . . . , x_{N}$. Each unit of the output map computes the maximum over some region (called a pooling group) of the input map. (Typically, the region could be 3$\\times$3.) In order to shrink the representation, we don’t consider all offsets, but instead we space them by a stride S along each dimension. This results in the representation being shrunk by a factor of approximately S along each dimension. (A typical value for the stride is 2.) Figure 7 shows an example of how pooling can provide partial invariance to translations of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79c14c5",
   "metadata": {},
   "source": [
    "<img src=\"03_images/03_Pooling_Figure7.PNG\" width=550px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30ae631",
   "metadata": {},
   "source": [
    "Pooling also has the effect of increasing the size of units’ receptive fields, or the regions of the input image which influence their activations. For instance, consider the network architecture in Figure 8, which alternates between convolution and pooling layers. Suppose all the filters are 5 $\\times$ 5 and the pooling layer uses a stride of 2. Then each unit in the first convolution layer has a receptive field of size 5 $\\times$ 5. But each unit in the second convolution layer has a receptive field of size approximately 10 $\\times$ 10, since it does 5 $\\times$ 5 filtering over a representation which was shrunken by a factor of 2 along each dimension. A third convolution layer would have 20 $\\times$ 20 receptive fields. Hence, pooling allows small filters to account for information over large regions of an image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86278951",
   "metadata": {},
   "source": [
    "<img src=\"03_images/03_Pooling_Figure8.PNG\" width=550px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c9f8d0",
   "metadata": {},
   "source": [
    "## Fully Connected Layer (FC Layer)\n",
    "\n",
    "Adding a Fully-Connected layer is a (usually) cheap way of learning non-linear combinations of the high-level features as represented by the output of the convolutional layer. The Fully-Connected layer is learning a possibly non-linear function in that space.\n",
    "\n",
    "Now that we have converted our input image into a suitable form for our Multi-Level Perceptron, we shall flatten the image into a column vector. The flattened output is fed to a feed-forward neural network and backpropagation applied to every iteration of training. Over a series of epochs, the model is able to distinguish between dominating and certain low-level features in images and classify them using the Softmax Classification technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a2a047",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "[1] Saha, S. (2018, December 17). A Comprehensive Guide to Convolutional Neural Networks - the ELI5 way. Medium. https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43bee38",
   "metadata": {},
   "source": [
    "[2] Grosse, R. (2019, February 5). Lecture 9: Convolutional Networks. http://www.cs.toronto.edu/~rgrosse/courses/csc421_2019/readings/L09%20Convolutional%20Networks.pdf. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2 (default, Dec 21 2020, 15:06:04) \n[Clang 12.0.0 (clang-1200.0.32.29)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
